{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/vondrick2/revant/conda_installation/miniconda3/envs/flava/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_vpt_clip import clip\n",
    "\n",
    "from deep_vpt_clip.prompters import TokenPrompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "add_prompt_len = 20\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/32', device, jit=False, prompt_len=add_prompt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "vision_transformer_layers = model.vision_layers\n",
    "print(vision_transformer_layers)\n",
    "\n",
    "deep_prompt = TokenPrompter(add_prompt_len, vision_transformer_layers)\n",
    "deep_prompt = deep_prompt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_tokens.shape:  torch.Size([12, 1, 20, 768])\n",
      "repeated prompt_tokens.shape:  torch.Size([12, 16, 20, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "x.shape:  torch.Size([70, 16, 768])\n",
      "this_layer_prompts.shape:  torch.Size([20, 16, 768])\n",
      "x shape after adding token torch.Size([70, 16, 768])\n",
      "image_features shape torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "# with automatic mixed precision\n",
    "with autocast():\n",
    "    prompt_tokens = deep_prompt()\n",
    "    print(\"prompt_tokens.shape: \", prompt_tokens.shape)\n",
    "    # random imag tensor\n",
    "    images_tensor = torch.rand((16, 3, 224, 224))\n",
    "    images_tensor = images_tensor.to(device)\n",
    "\n",
    "    if prompt_tokens is not None:\n",
    "        bs = images_tensor.size(0)\n",
    "        prompt_tokens = prompt_tokens.repeat(1, bs, 1, 1)\n",
    "\n",
    "    print(\"repeated prompt_tokens.shape: \", prompt_tokens.shape)\n",
    "\n",
    "    image_features = model.encode_image(images_tensor, prompt_tokens)\n",
    "    # image_features = model.encode_image(images_tensor, None)\n",
    "\n",
    "    print(\"image_features shape\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('flava')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4.0,
  "vscode": {
   "interpreter": {
    "hash": "70457002a72f6cf4133645646caca2657e9253b1a44255715284cc1f69725b1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
